{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10725266,"sourceType":"datasetVersion","datasetId":6648899}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes jiwer datasets peft loralib tqdm pytesseract\n!apt-get install tesseract-ocr\n!apt-get install tesseract-ocr-eng","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import ( LlamaForCausalLM, LlamaTokenizer, AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig )\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport jiwer\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom glob import glob\nimport random\nimport pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:13:02.267955Z","iopub.execute_input":"2025-02-13T17:13:02.268252Z","iopub.status.idle":"2025-02-13T17:13:02.273080Z","shell.execute_reply.started":"2025-02-13T17:13:02.268230Z","shell.execute_reply":"2025-02-13T17:13:02.272327Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:13:05.051616Z","iopub.execute_input":"2025-02-13T17:13:05.051972Z","iopub.status.idle":"2025-02-13T17:13:05.356460Z","shell.execute_reply.started":"2025-02-13T17:13:05.051944Z","shell.execute_reply":"2025-02-13T17:13:05.355538Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:13:08.510992Z","iopub.execute_input":"2025-02-13T17:13:08.511275Z","iopub.status.idle":"2025-02-13T17:13:08.515745Z","shell.execute_reply.started":"2025-02-13T17:13:08.511255Z","shell.execute_reply":"2025-02-13T17:13:08.514863Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"\n\n# Load Images from the folder","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    for filename in os.listdir(folder):\n        if filename.endswith(\".jpg\"):\n            img = Image.open(os.path.join(folder, filename)).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n    return images, image_names\n\nimage_folder = \"/kaggle/input/dataset/images\"\ndataset, image_names = load_images_from_folder(image_folder)\nprint(\"Dataset loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:13:12.317340Z","iopub.execute_input":"2025-02-13T17:13:12.317633Z","iopub.status.idle":"2025-02-13T17:13:15.954353Z","shell.execute_reply.started":"2025-02-13T17:13:12.317611Z","shell.execute_reply":"2025-02-13T17:13:15.953429Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Generate True text from the images using OCR\ntakes about 12-15 min to generate text","metadata":{}},{"cell_type":"code","source":"def generate_ground_truth(images, image_names):\n    ground_truth = {}\n    for img, name in zip(images, image_names):\n        text = pytesseract.image_to_string(img).strip()\n        if not text:  # If OCR fails to extract text, use a placeholder\n            text = \"N/A\"\n        ground_truth[name] = text\n    return ground_truth\n\nground_truth_data = generate_ground_truth(dataset, image_names)\ntrue_texts = [ground_truth_data[img_name] for img_name in image_names]\nprint(\"true_texts generated\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:13:20.654050Z","iopub.execute_input":"2025-02-13T17:13:20.654359Z","iopub.status.idle":"2025-02-13T17:23:32.735610Z","shell.execute_reply.started":"2025-02-13T17:13:20.654336Z","shell.execute_reply":"2025-02-13T17:23:32.734849Z"}},"outputs":[{"name":"stdout","text":"true_texts generated\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"ground_truth_data['india_news_p000060.jpg']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating and Loading the model \"Llama-3.2-11B-Vision\"\nLoaded a 4-bit quantized llama-3.2-11B-vision model. *Note: Loading the model can take upto 10 minutes.*","metadata":{}},{"cell_type":"code","source":"def load_model():\n    model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",  # normalized float 4\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:42:29.163318Z","iopub.execute_input":"2025-02-13T16:42:29.163609Z","iopub.status.idle":"2025-02-13T16:42:29.168048Z","shell.execute_reply.started":"2025-02-13T16:42:29.163588Z","shell.execute_reply":"2025-02-13T16:42:29.167176Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model, processor = load_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:42:32.543765Z","iopub.execute_input":"2025-02-13T16:42:32.544197Z","iopub.status.idle":"2025-02-13T16:52:43.742402Z","shell.execute_reply.started":"2025-02-13T16:42:32.544159Z","shell.execute_reply":"2025-02-13T16:52:43.741730Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/5.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7e1a5177ba4898acab399e1b914689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b022646bcec94f7c90c30c5dfb2c8788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83fd60b3aa2d4f0e8f755686353b33c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c544de4a2e4671987c4afa6a9e2dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3a8d8f82e048b4a8852ca659c4834a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44c3d505865492290c781b6b7316f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08b46053a554c70a023698aa6e39634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9b2ec7937e425cb44589df2b42d8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65575d5e89364328871e9c37943ee4f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be6cb5556c54501ad94ef62c8ada7d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42edf26b8c1547d7ab2ed4d94dd94e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7efb45c4c6648579a3b6a153c03c9cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61ae79ec2be40629d136e74839a4997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15da9414acb44d769ed3c3d22e69c839"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Extract baseline texts using the model\nTook about 5 hours to extract text using the model.","metadata":{}},{"cell_type":"code","source":"def extract_text(images,model,processor):\n    texts = []\n    for img in tqdm(images):\n        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n        outputs = model.generate(**inputs,max_length=512,num_beams=5,early_stopping=True)\n        text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n        texts.append(text)\n    return texts\n\nbaseline_texts = extract_text(dataset,model,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Organization\nCleaning the texts using regular expression for removing '\\n' and some unnecessary characters","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt_tab')\n\ndef clean_text(text):\n    text = text.strip()  # Remove leading and trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n    text = re.sub(r'\\n+', ' ', text)  # Remove excessive newlines\n    text = text.replace(\"  \", \" \")  # Remove double spaces\n    return text\n\ndef structure_text(text):\n    sentences = sent_tokenize(text)  # Tokenize into sentences\n    structured_text = \"\\n\".join(sentences)  # Join sentences with newline\n    return structured_text\n\ndef process_extracted_text(extracted_texts):\n    organized_texts = []\n    for text in extracted_texts:\n        cleaned = clean_text(text)\n        structured = structure_text(cleaned)\n        organized_texts.append(structured)\n    \n    return organized_texts\n\norganized_texts = process_extracted_text(baseline_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating texts i.e. calculating words error and character error","metadata":{}},{"cell_type":"code","source":"from jiwer import wer, cer\ndef evaluate_texts(true_texts, predicted_texts):\n    word_error = wer(true_texts, predicted_texts)\n    char_error = cer(true_texts, predicted_texts)\n    return word_error, char_error\n\nword_error, char_error = evaluate_texts(true_texts, organized_texts)\nprint(f\"Word Error Rate: {word_error}\")\nprint(f\"Character Error Rate: {char_error}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tune the model using LoRA (Low Rank Adaptation)","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, images, texts, processor):\n        self.images = images\n        self.texts = texts\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        text = self.texts[idx]\n        \n        # Format prompt with the correct image token format for LLaMA Vision\n        prompt = \"<image>\\nExtract and describe the text in this image.\\n</image>\\nText in the image:\"\n        \n        # First process the image\n        image_inputs = self.processor.image_processor(images=image, return_tensors=\"pt\")\n        \n        # Then process the text with the correct token\n        text_inputs = self.processor.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            add_special_tokens=True,\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n        \n        # Combine inputs\n        inputs = {\n            \"pixel_values\": image_inputs.pixel_values[0],\n            \"input_ids\": text_inputs.input_ids[0],\n            \"attention_mask\": text_inputs.attention_mask[0]\n        }\n        \n        # Add labels for training\n        label_inputs = self.processor.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n        \n        inputs[\"labels\"] = label_inputs.input_ids[0]\n        \n        return inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:30:41.409607Z","iopub.execute_input":"2025-02-13T17:30:41.409987Z","iopub.status.idle":"2025-02-13T17:30:41.416473Z","shell.execute_reply.started":"2025-02-13T17:30:41.409961Z","shell.execute_reply":"2025-02-13T17:30:41.415466Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_images, val_images, train_texts, val_texts = train_test_split(\n    dataset, true_texts, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:30:48.122571Z","iopub.execute_input":"2025-02-13T17:30:48.122944Z","iopub.status.idle":"2025-02-13T17:30:48.127787Z","shell.execute_reply.started":"2025-02-13T17:30:48.122912Z","shell.execute_reply":"2025-02-13T17:30:48.126935Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"train_dataset = ImageTextDataset(train_images, train_texts, processor)\nval_dataset = ImageTextDataset(val_images, val_texts, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:30:53.701617Z","iopub.execute_input":"2025-02-13T17:30:53.701983Z","iopub.status.idle":"2025-02-13T17:30:53.705800Z","shell.execute_reply.started":"2025-02-13T17:30:53.701955Z","shell.execute_reply":"2025-02-13T17:30:53.704892Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def collate_fn(examples):\n    batch = {\n        \"pixel_values\": torch.stack([example[\"pixel_values\"] for example in examples]),\n        \"input_ids\": torch.stack([example[\"input_ids\"] for example in examples]),\n        \"attention_mask\": torch.stack([example[\"attention_mask\"] for example in examples]),\n        \"labels\": torch.stack([example[\"labels\"] for example in examples])\n    }\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:30:58.543271Z","iopub.execute_input":"2025-02-13T17:30:58.543553Z","iopub.status.idle":"2025-02-13T17:30:58.548069Z","shell.execute_reply.started":"2025-02-13T17:30:58.543531Z","shell.execute_reply":"2025-02-13T17:30:58.547152Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,  # rank\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# Add LoRA adapters\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:31:00.433778Z","iopub.execute_input":"2025-02-13T17:31:00.434069Z","iopub.status.idle":"2025-02-13T17:31:00.783625Z","shell.execute_reply.started":"2025-02-13T17:31:00.434048Z","shell.execute_reply":"2025-02-13T17:31:00.782974Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./llama-vision-finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_steps=10,\n    fp16=True,\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:31:01.373691Z","iopub.execute_input":"2025-02-13T17:31:01.374016Z","iopub.status.idle":"2025-02-13T17:31:01.400842Z","shell.execute_reply.started":"2025-02-13T17:31:01.373992Z","shell.execute_reply":"2025-02-13T17:31:01.400174Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:31:02.779598Z","iopub.execute_input":"2025-02-13T17:31:02.779933Z","iopub.status.idle":"2025-02-13T17:31:02.791939Z","shell.execute_reply.started":"2025-02-13T17:31:02.779903Z","shell.execute_reply":"2025-02-13T17:31:02.791213Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
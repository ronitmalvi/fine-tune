{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10725266,"sourceType":"datasetVersion","datasetId":6648899}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes jiwer datasets peft loralib tqdm pytesseract opencv-python\n!apt-get install tesseract-ocr\n!apt-get install tesseract-ocr-eng","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-25T17:55:59.868642Z","iopub.execute_input":"2025-03-25T17:55:59.868962Z","iopub.status.idle":"2025-03-25T17:56:14.804442Z","shell.execute_reply.started":"2025-03-25T17:55:59.868931Z","shell.execute_reply":"2025-03-25T17:56:14.803329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import ( LlamaForCausalLM, LlamaTokenizer, AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig )\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport jiwer\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom glob import glob\nimport random\nimport pytesseract\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:20.168527Z","iopub.execute_input":"2025-03-25T17:56:20.168866Z","iopub.status.idle":"2025-03-25T17:56:44.076160Z","shell.execute_reply.started":"2025-03-25T17:56:20.168839Z","shell.execute_reply":"2025-03-25T17:56:44.075489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:45.970372Z","iopub.execute_input":"2025-03-25T17:56:45.971047Z","iopub.status.idle":"2025-03-25T17:56:46.170504Z","shell.execute_reply.started":"2025-03-25T17:56:45.971016Z","shell.execute_reply":"2025-03-25T17:56:46.169850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:49.792286Z","iopub.execute_input":"2025-03-25T17:56:49.792721Z","iopub.status.idle":"2025-03-25T17:56:49.887277Z","shell.execute_reply.started":"2025-03-25T17:56:49.792686Z","shell.execute_reply":"2025-03-25T17:56:49.886238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# Load Images from the folder and Cropping them","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    for filename in os.listdir(folder):\n        if filename.endswith(\".jpg\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n    return images, image_names\n\nimage_folder = \"/kaggle/input/dataset/images\"\ndataset, image_names = load_images_from_folder(image_folder)\nprint(\"Loaded images successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:51.592824Z","iopub.execute_input":"2025-03-25T17:56:51.593175Z","iopub.status.idle":"2025-03-25T17:56:57.943583Z","shell.execute_reply.started":"2025-03-25T17:56:51.593150Z","shell.execute_reply":"2025-03-25T17:56:57.942781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_images(image_names):\n    for j,img in enumerate(image_names):\n        image_path = f\"/kaggle/input/dataset/images/{img}\"  # Change this to your image path\n        image = cv2.imread(image_path)\n        \n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        \n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n        \n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))  # Adjust based on text density\n        dilated = cv2.dilate(clean, kernel, iterations=2)\n        \n        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        bounding_boxes = [cv2.boundingRect(cnt) for cnt in contours]\n        bounding_boxes = sorted(bounding_boxes, key=lambda b: b[1])\n        \n        merged_boxes = []\n        i = 0\n        \n        while i < len(bounding_boxes) - 1:\n            x1, y1, w1, h1 = bounding_boxes[i]\n            x2, y2, w2, h2 = bounding_boxes[i + 1]\n        \n            # Check if two bounding boxes are close enough to be considered part of the same section\n            if abs(y2 - (y1 + h1)) < 50:  # Merge if next box is within 50 pixels\n                x_new = min(x1, x2)\n                y_new = min(y1, y2)\n                w_new = max(x1 + w1, x2 + w2) - x_new\n                h_new = max(y1 + h1, y2 + h2) - y_new\n        \n                merged_boxes.append((x_new, y_new, w_new, h_new))\n                i += 2\n            else:\n                i += 1\n        \n        output_dir = \"/kaggle/working/cropped_paragraphs\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        min_area = 10000  # Minimum bounding box area (to ignore single words)\n        min_height = 60   # Minimum height of a paragraph block\n        min_aspect_ratio = 0.5  # To avoid very long but short-height text (headers, footers)\n        \n        for i, (x, y, w, h) in enumerate(merged_boxes):\n            aspect_ratio = w / h \n        \n            if h > min_height and w * h > min_area and aspect_ratio > min_aspect_ratio:\n                cropped_paragraph = image[y:y+h, x:x+w]  # Crop paragraph\n                save_path = os.path.join(output_dir, f\"multi_paragraph_{j}_{i+1}.png\")\n                cv2.imwrite(save_path, cropped_paragraph)\n        \n                # cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        half_count = len(contours) // 3\n        j=1\n        for i, cnt in enumerate(contours):\n            x, y, w, h = cv2.boundingRect(cnt)\n            \n            \n            aspect_ratio = w / h \n        \n            if h > min_height and w * h > min_area and aspect_ratio > min_aspect_ratio:\n                cropped_paragraph = image[y:y+h, x:x+w]  # Crop paragraph\n                save_path = os.path.join(output_dir, f\"paragraph_{j}_{i+1}.png\")\n                cv2.imwrite(save_path, cropped_paragraph)\n                j=j+1\n            if j==half_count:\n                break\n                # Draw bounding box on original image (for visualization)\n                # cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:01.507196Z","iopub.execute_input":"2025-03-25T17:57:01.507524Z","iopub.status.idle":"2025-03-25T17:57:01.518227Z","shell.execute_reply.started":"2025-03-25T17:57:01.507499Z","shell.execute_reply":"2025-03-25T17:57:01.517299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augment_images(image_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:03.188311Z","iopub.execute_input":"2025-03-25T17:57:03.188615Z","iopub.status.idle":"2025-03-25T17:57:25.726621Z","shell.execute_reply.started":"2025-03-25T17:57:03.188591Z","shell.execute_reply":"2025-03-25T17:57:25.725703Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate True text from the images using OCR\ntakes about 12-15 min to generate text","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    cnt=0\n    for filename in os.listdir(folder):\n        if filename.endswith(\".png\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n            cnt=cnt+1\n    return images, image_names,cnt\n\nimage_folder = \"/kaggle/working/cropped_paragraphs\"\ndataset, image_names,cnt = load_images_from_folder(image_folder)\nprint(f\"Loaded images successfully: {cnt}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:32.259600Z","iopub.execute_input":"2025-03-25T17:57:32.259921Z","iopub.status.idle":"2025-03-25T17:57:39.565980Z","shell.execute_reply.started":"2025-03-25T17:57:32.259895Z","shell.execute_reply":"2025-03-25T17:57:39.565223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_ground_truth(images, image_names):\n    ground_truth = {}\n    for img, name in tqdm(zip(images, image_names), total=len(image_names), desc=\"Processing Images\"):\n        text = pytesseract.image_to_string(img).strip()\n        if not text:  # If OCR fails\n            text = \"N/A\"\n        ground_truth[name] = text\n    return ground_truth\n\nground_truth_data = generate_ground_truth(dataset, image_names)\nprint(\"true_texts generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:09:52.728599Z","iopub.execute_input":"2025-03-25T17:09:52.728978Z","iopub.status.idle":"2025-03-25T17:23:28.959409Z","shell.execute_reply.started":"2025-03-25T17:09:52.728949Z","shell.execute_reply":"2025-03-25T17:23:28.958382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating and Loading the model \"Llama-3.2-11B-Vision\"\nLoaded a 4-bit quantized llama-3.2-11B-vision model. *Note: Loading the model can take upto 4 minutes as the model is 22GB big to download*","metadata":{}},{"cell_type":"code","source":"def load_model():\n    model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:45.535869Z","iopub.execute_input":"2025-03-25T17:57:45.536207Z","iopub.status.idle":"2025-03-25T17:57:45.540677Z","shell.execute_reply.started":"2025-03-25T17:57:45.536180Z","shell.execute_reply":"2025-03-25T17:57:45.539861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, processor = load_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:47.993260Z","iopub.execute_input":"2025-03-25T17:57:47.993562Z","iopub.status.idle":"2025-03-25T18:02:10.207869Z","shell.execute_reply.started":"2025-03-25T17:57:47.993540Z","shell.execute_reply":"2025-03-25T18:02:10.207096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Extract baseline texts using the model\n* Took about 5 hours to extract text using the model if the max_length put to be 128.","metadata":{}},{"cell_type":"code","source":"from torch.amp import autocast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:29.762602Z","iopub.execute_input":"2025-03-25T18:02:29.763309Z","iopub.status.idle":"2025-03-25T18:02:29.767146Z","shell.execute_reply.started":"2025-03-25T18:02:29.763278Z","shell.execute_reply":"2025-03-25T18:02:29.766088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:30.838082Z","iopub.execute_input":"2025-03-25T18:02:30.838455Z","iopub.status.idle":"2025-03-25T18:02:30.842108Z","shell.execute_reply.started":"2025-03-25T18:02:30.838426Z","shell.execute_reply":"2025-03-25T18:02:30.841302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text(images, model, processor, batch_size=4):\n    texts = []\n    \n    device = next(model.parameters()).device\n    \n    for i in tqdm(range(0, len(images), batch_size)):\n        batch = images[i:i+batch_size]\n        \n        inputs = processor(images=batch, return_tensors=\"pt\", padding=True).to(device)\n        \n        with autocast('cuda'):\n            outputs = model.generate(**inputs, max_length=128, num_beams=2, early_stopping=True)\n        \n        batch_texts = processor.batch_decode(outputs, skip_special_tokens=True)\n        texts.extend(batch_texts)\n    \n    return texts\n\nbaseline_texts = extract_text(dataset, model, processor, batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:34.066669Z","iopub.execute_input":"2025-03-25T18:02:34.067022Z","iopub.status.idle":"2025-03-25T18:02:47.471922Z","shell.execute_reply.started":"2025-03-25T18:02:34.066978Z","shell.execute_reply":"2025-03-25T18:02:47.470514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Organization\nCleaning the texts using regular expression for removing some unnecessary characters","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt_tab')\n\ndef clean_text(text):\n    text = text.strip()  # Remove leading and trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n    text = re.sub(r'\\n+', ' ', text)  # Remove excessive newlines\n    text = text.replace(\"  \", \" \")  # Remove double spaces\n    return text\n\ndef structure_text(text):\n    sentences = sent_tokenize(text)  # Tokenize into sentences\n    structured_text = \"\\n\".join(sentences)  # Join sentences with newline\n    return structured_text\n\ndef process_extracted_text(extracted_texts):\n    organized_texts = []\n    for text in extracted_texts:\n        cleaned = clean_text(text)\n        structured = structure_text(cleaned)\n        organized_texts.append(structured)\n    \n    return organized_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_list(data, batch_size):\n    return [\"\".join(data[i:i + batch_size]) for i in range(0, len(data), batch_size)]\n    \nbatch_true_text=batch_list(true_texts,4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_true_texts = process_extracted_text(batch_true_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_predicted_texts=process_extracted_text(baseline_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating texts i.e. calculating words error and character error\n* The error comes to be quite high because the length of the text generated by the model is much less than the length of actual text as extrated by OCR.","metadata":{}},{"cell_type":"code","source":"!pip install jiwer\nfrom jiwer import wer, cer\ndef evaluate_texts(true_texts, predicted_texts):\n    word_error = wer(true_texts, predicted_texts)\n    char_error = cer(true_texts, predicted_texts)\n    return word_error, char_error","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_wer,baseline_cer = evaluate_texts(true_texts, predicted_texts)\nprint(f\"Word Error Rate: {baseline_wer}\")\nprint(f\"Character Error Rate: {baseline_cer}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tune the model using LoRA (Low Rank Adaptation)\n* training the model and obtaining the fine_tuned model took quite longer approax. 6-7 hours ","metadata":{}},{"cell_type":"code","source":"def prepare_dataset_batch(images, true_texts, processor, batch_size=4):\n    pixel_values = []\n    input_ids = []\n    \n    # Process images in batches\n    for i in range(0, len(images), batch_size):\n        batch_images = images[i:i+batch_size]\n        batch_texts = [\"\".join(true_texts[i:i + batch_size])]\n        \n        # Process image batch\n        image_features = processor(\n            images=batch_images, \n            return_tensors=\"pt\",\n            padding=True\n        )\n        pixel_values.extend(image_features[\"pixel_values\"])\n        \n        # Process text batch\n        text_features = processor(\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n        input_ids.extend(text_features[\"input_ids\"])\n        \n    print(len(pixel_values))\n    print(len(input_ids))\n    return Dataset.from_dict({\n        \"pixel_values\": pixel_values,\n        \"labels\": input_ids\n    }).train_test_split(test_size=0.1, seed=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def configure_lora(model):\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback, TrainingArguments, Trainer\nfrom tqdm import tqdm\n\nclass ProgressBarCallback(TrainerCallback):\n    def __init__(self, total_steps):\n        self.pbar = tqdm(total=total_steps, desc=\"Training Progress\")\n\n    def on_step_end(self, args, state, control, **kwargs):\n        self.pbar.update(1)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        self.pbar.close()\n\ndef train_model(model, dataset, processor):\n    training_args = TrainingArguments(\n        output_dir=\"./llama-vision-finetuned\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=2,\n        dataloader_num_workers=2,\n        learning_rate=2e-4,\n        weight_decay=0.01,\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        push_to_hub=False,\n        remove_unused_columns=False\n    )\n\n    total_steps = (len(dataset[\"train\"]) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n    progress_callback = ProgressBarCallback(total_steps)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=collate_fn,\n        callbacks=[progress_callback]  # Attach the progress bar\n    )\n\n    trainer.train()\n    return trainer, model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset\ndataset = prepare_dataset_batch(cropped_dataset, true_texts, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure and fine-tune model\nmodel_finetuned = configure_lora(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, model_finetuned = train_model(model_finetuned, dataset, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_texts=extract_text(cropped_dataset,model_finetuned,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"organised_finetuned_texts=process_extracted_text(finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_wer, finetuned_cer = evaluate_texts(true_texts, organized_finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Baseline Model Performance:\")\nprint(f\"Word Error Rate: {baseline_wer:.4f}\")\nprint(f\"Character Error Rate: {baseline_cer:.4f}\")\n\nprint(\"\\nFine-tuned Model Performance:\")\nprint(f\"Word Error Rate: {finetuned_wer:.4f}\")\nprint(f\"Character Error Rate: {finetuned_cer:.4f}\")\n\nprint(\"\\nImprovement:\")\nprint(f\"Word Error Rate Improvement: {(baseline_wer - finetuned_wer) * 100:.2f}%\")\nprint(f\"Character Error Rate Improvement: {(baseline_cer - finetuned_cer) * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
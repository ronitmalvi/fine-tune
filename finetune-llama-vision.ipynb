{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10725266,"sourceType":"datasetVersion","datasetId":6648899}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes jiwer datasets peft loralib tqdm pytesseract\n!apt-get install tesseract-ocr\n!apt-get install tesseract-ocr-eng","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import ( LlamaForCausalLM, LlamaTokenizer, AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig )\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport jiwer\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom glob import glob\nimport random\nimport pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:25:24.784454Z","iopub.execute_input":"2025-02-14T07:25:24.784815Z","iopub.status.idle":"2025-02-14T07:25:47.295594Z","shell.execute_reply.started":"2025-02-14T07:25:24.784784Z","shell.execute_reply":"2025-02-14T07:25:47.294705Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:09:04.861993Z","iopub.execute_input":"2025-02-13T18:09:04.862673Z","iopub.status.idle":"2025-02-13T18:09:05.191352Z","shell.execute_reply.started":"2025-02-13T18:09:04.862642Z","shell.execute_reply":"2025-02-13T18:09:05.190491Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:09:08.431604Z","iopub.execute_input":"2025-02-13T18:09:08.431933Z","iopub.status.idle":"2025-02-13T18:09:08.484944Z","shell.execute_reply.started":"2025-02-13T18:09:08.431906Z","shell.execute_reply":"2025-02-13T18:09:08.484168Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"\n\n# Load Images from the folder","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\ndef load_images_from_folder(folder):\n    images = []\n    image_names = []\n    for filename in os.listdir(folder):\n        if filename.endswith(\".jpg\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n    return images, image_names\n\ndef crop_and_save_images(images, image_names, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)  # Create folder if it doesn't exist\n\n    cropped_images = []\n    cropped_image_names = []\n\n    for img, filename in zip(images, image_names):\n        width, height = img.size\n        left = width // 22\n        upper = 0\n        right = width\n        lower = height - (height // 12.2)\n        crop_box = (left, upper, right, lower)\n\n        cropped_img = img.crop(crop_box)\n        \n        # Save cropped image\n        cropped_img_path = os.path.join(output_folder, filename)\n        cropped_img.save(cropped_img_path)\n\n        cropped_images.append(cropped_img)\n        cropped_image_names.append(filename)\n\n    return cropped_images, cropped_image_names\n\n# Paths\nimage_folder = \"/kaggle/input/dataset/images\"\ncropped_image_folder = \"/kaggle/working/cropped_images\"\n\n# Load images\ndataset, image_names = load_images_from_folder(image_folder)\n\n# Crop images and create dataset\ncropped_dataset, cropped_image_names = crop_and_save_images(dataset, image_names, cropped_image_folder)\n\nprint(\"Cropped dataset created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:09:11.521934Z","iopub.execute_input":"2025-02-13T18:09:11.522288Z","iopub.status.idle":"2025-02-13T18:09:20.119444Z","shell.execute_reply.started":"2025-02-13T18:09:11.522258Z","shell.execute_reply":"2025-02-13T18:09:20.118540Z"}},"outputs":[{"name":"stdout","text":"Cropped dataset created successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Generate True text from the images using OCR\ntakes about 12-15 min to generate text","metadata":{}},{"cell_type":"code","source":"def generate_ground_truth(images, image_names):\n    ground_truth = {}\n    for img, name in zip(images, image_names):\n        text = pytesseract.image_to_string(img).strip()\n        if not text:  # If OCR fails to extract text, use a placeholder\n            text = \"N/A\"\n        ground_truth[name] = text\n    return ground_truth\n\nground_truth_data = generate_ground_truth(cropped_dataset, cropped_image_names)\ntrue_texts = [ground_truth_data[img_name] for img_name in image_names]\nprint(\"true_texts generated\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:09:25.826204Z","iopub.execute_input":"2025-02-13T18:09:25.826549Z","iopub.status.idle":"2025-02-13T18:19:24.972369Z","shell.execute_reply.started":"2025-02-13T18:09:25.826520Z","shell.execute_reply":"2025-02-13T18:19:24.971364Z"}},"outputs":[{"name":"stdout","text":"true_texts generated\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"ground_truth_data['india_news_p000060.jpg']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating and Loading the model \"Llama-3.2-11B-Vision\"\nLoaded a 4-bit quantized llama-3.2-11B-vision model. *Note: Loading the model can take upto 10 minutes.*","metadata":{}},{"cell_type":"code","source":"def load_model():\n    model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",  # normalized float 4\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:04.293965Z","iopub.execute_input":"2025-02-13T18:20:04.294340Z","iopub.status.idle":"2025-02-13T18:20:04.298629Z","shell.execute_reply.started":"2025-02-13T18:20:04.294310Z","shell.execute_reply":"2025-02-13T18:20:04.297754Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model, processor = load_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract baseline texts using the model\nTook about 5 hours to extract text using the model.","metadata":{}},{"cell_type":"code","source":"def extract_text(images,model,processor):\n    texts = []\n    for img in tqdm(images):\n        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n        outputs = model.generate(**inputs,max_length=256,num_beams=5,early_stopping=True)\n        text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n        texts.append(text)\n    return texts\n\nbaseline_texts = extract_text(cropped_dataset,model,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Organization\nCleaning the texts using regular expression for removing '\\n' and some unnecessary characters","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt_tab')\n\ndef clean_text(text):\n    text = text.strip()  # Remove leading and trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n    text = re.sub(r'\\n+', ' ', text)  # Remove excessive newlines\n    text = text.replace(\"  \", \" \")  # Remove double spaces\n    return text\n\ndef structure_text(text):\n    sentences = sent_tokenize(text)  # Tokenize into sentences\n    structured_text = \"\\n\".join(sentences)  # Join sentences with newline\n    return structured_text\n\ndef process_extracted_text(extracted_texts):\n    organized_texts = []\n    for text in extracted_texts:\n        cleaned = clean_text(text)\n        structured = structure_text(cleaned)\n        organized_texts.append(structured)\n    \n    return organized_texts\n\norganized_texts = process_extracted_text(baseline_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating texts i.e. calculating words error and character error","metadata":{}},{"cell_type":"code","source":"from jiwer import wer, cer\ndef evaluate_texts(true_texts, predicted_texts):\n    word_error = wer(true_texts, predicted_texts)\n    char_error = cer(true_texts, predicted_texts)\n    return word_error, char_error\n\nword_error, char_error = evaluate_texts(true_texts, organized_texts)\nprint(f\"Word Error Rate: {word_error}\")\nprint(f\"Character Error Rate: {char_error}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tune the model using LoRA (Low Rank Adaptation)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
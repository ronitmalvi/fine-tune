{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10725266,"sourceType":"datasetVersion","datasetId":6648899}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes jiwer datasets peft loralib tqdm pytesseract\n!apt-get install tesseract-ocr\n!apt-get install tesseract-ocr-eng","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import ( LlamaForCausalLM, LlamaTokenizer, AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig )\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport jiwer\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom glob import glob\nimport random\nimport pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:43:28.657039Z","iopub.execute_input":"2025-02-14T07:43:28.657343Z","iopub.status.idle":"2025-02-14T07:43:28.662946Z","shell.execute_reply.started":"2025-02-14T07:43:28.657320Z","shell.execute_reply":"2025-02-14T07:43:28.661842Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:43:31.133259Z","iopub.execute_input":"2025-02-14T07:43:31.133591Z","iopub.status.idle":"2025-02-14T07:43:31.432495Z","shell.execute_reply.started":"2025-02-14T07:43:31.133564Z","shell.execute_reply":"2025-02-14T07:43:31.431783Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:43:34.095555Z","iopub.execute_input":"2025-02-14T07:43:34.095895Z","iopub.status.idle":"2025-02-14T07:43:34.101139Z","shell.execute_reply.started":"2025-02-14T07:43:34.095868Z","shell.execute_reply":"2025-02-14T07:43:34.100094Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"\n\n# Load Images from the folder and Cropping them","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    for filename in os.listdir(folder):\n        if filename.endswith(\".jpg\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n    return images, image_names\n\nimage_folder = \"/kaggle/input/dataset/images\"\ndataset, image_names = load_images_from_folder(image_folder)\nprint(\"Loaded images successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:43:37.278380Z","iopub.execute_input":"2025-02-14T07:43:37.278686Z","iopub.status.idle":"2025-02-14T07:43:45.196117Z","shell.execute_reply.started":"2025-02-14T07:43:37.278664Z","shell.execute_reply":"2025-02-14T07:43:45.195182Z"}},"outputs":[{"name":"stdout","text":"Cropped dataset created successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def crop_and_save_images(images, image_names, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    cropped_images = []\n    cropped_image_names = []\n\n    for img, filename in zip(images, image_names):\n        width, height = img.size\n        left = width // 22\n        upper = 0\n        right = width\n        lower = height - (height // 12.2)\n        crop_box = (left, upper, right, lower)\n\n        cropped_img = img.crop(crop_box)\n        \n        cropped_img_path = os.path.join(output_folder, filename)\n        cropped_img.save(cropped_img_path)\n\n        cropped_images.append(cropped_img)\n        cropped_image_names.append(filename)\n\n    return cropped_images, cropped_image_names\n\ncropped_image_folder = \"/kaggle/working/cropped_images\"\n\ncropped_dataset, cropped_image_names = crop_and_save_images(dataset, image_names, cropped_image_folder)\n\nprint(\"Cropped dataset created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate True text from the images using OCR\ntakes about 12-15 min to generate text","metadata":{}},{"cell_type":"code","source":"def generate_ground_truth(images, image_names):\n    ground_truth = {}\n    for img, name in zip(images, image_names):\n        text = pytesseract.image_to_string(img).strip()\n        if not text:  # If OCR fails\n            text = \"N/A\"\n        ground_truth[name] = text\n    return ground_truth\n\nground_truth_data = generate_ground_truth(cropped_dataset, cropped_image_names)\ntrue_texts = [ground_truth_data[img_name] for img_name in image_names]\nprint(\"true_texts generated\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:43:50.141197Z","iopub.execute_input":"2025-02-14T07:43:50.141486Z","iopub.status.idle":"2025-02-14T07:53:52.772154Z","shell.execute_reply.started":"2025-02-14T07:43:50.141453Z","shell.execute_reply":"2025-02-14T07:53:52.771303Z"}},"outputs":[{"name":"stdout","text":"true_texts generated\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Creating and Loading the model \"Llama-3.2-11B-Vision\"\nLoaded a 4-bit quantized llama-3.2-11B-vision model. *Note: Loading the model can take upto 10 minutes.*","metadata":{}},{"cell_type":"code","source":"def load_model():\n    model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",  # normalized float 4\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:59:04.850378Z","iopub.execute_input":"2025-02-14T07:59:04.850727Z","iopub.status.idle":"2025-02-14T07:59:04.855259Z","shell.execute_reply.started":"2025-02-14T07:59:04.850698Z","shell.execute_reply":"2025-02-14T07:59:04.854379Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model, processor = load_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Extract baseline texts using the model\n* Took about 5 hours to extract text using the model if the max_length put to be 256.\n* And would take 10 hours for max_length=512.","metadata":{}},{"cell_type":"code","source":"def extract_text(images,model,processor):\n    texts = []\n    for img in tqdm(images):\n        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n        outputs = model.generate(**inputs,max_length=256,num_beams=5,early_stopping=True)\n        text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n        texts.append(text)\n    return texts\n\nbaseline_texts = extract_text(cropped_dataset,model,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Organization\nCleaning the texts using regular expression for removing '\\n' and some unnecessary characters","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt_tab')\n\ndef clean_text(text):\n    text = text.strip()  # Remove leading and trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n    text = re.sub(r'\\n+', ' ', text)  # Remove excessive newlines\n    text = text.replace(\"  \", \" \")  # Remove double spaces\n    return text\n\ndef structure_text(text):\n    sentences = sent_tokenize(text)  # Tokenize into sentences\n    structured_text = \"\\n\".join(sentences)  # Join sentences with newline\n    return structured_text\n\ndef process_extracted_text(extracted_texts):\n    organized_texts = []\n    for text in extracted_texts:\n        cleaned = clean_text(text)\n        structured = structure_text(cleaned)\n        organized_texts.append(structured)\n    \n    return organized_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:10:00.245670Z","iopub.execute_input":"2025-02-14T08:10:00.245987Z","iopub.status.idle":"2025-02-14T08:10:00.279522Z","shell.execute_reply.started":"2025-02-14T08:10:00.245964Z","shell.execute_reply":"2025-02-14T08:10:00.278733Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"true_texts = process_extracted_text(true_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:56:31.668400Z","iopub.execute_input":"2025-02-14T07:56:31.668715Z","iopub.status.idle":"2025-02-14T07:56:31.829560Z","shell.execute_reply.started":"2025-02-14T07:56:31.668690Z","shell.execute_reply":"2025-02-14T07:56:31.828654Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"predicted_texts=process_extracted_text(baseline_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating texts i.e. calculating words error and character error\n* The error comes to be quite high because the length of the text generated by the model is much less than the length of actual text as extrated by OCR.","metadata":{}},{"cell_type":"code","source":"from jiwer import wer, cer\ndef evaluate_texts(true_texts, predicted_texts):\n    word_error = wer(true_texts, predicted_texts)\n    char_error = cer(true_texts, predicted_texts)\n    return word_error, char_error\n\nbaseline_wer,baseline_cer = evaluate_texts(true_texts, organized_texts)\nprint(f\"Word Error Rate: {word_error}\")\nprint(f\"Character Error Rate: {char_error}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tune the model using LoRA (Low Rank Adaptation)\n* training the model and obtaining the fine_tuned model took quite longer approax. 6-7 hours ","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(images, true_texts, processor):\n    pixel_values_list = []\n    input_ids_list = []\n    \n    for img, text in zip(images, true_texts):\n        image_features = processor(images=img, return_tensors=\"pt\")\n        pixel_values_list.append(image_features[\"pixel_values\"][0])\n        \n        text_features = processor(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=256)\n        input_ids_list.append(text_features[\"input_ids\"][0])\n    \n    dataset_dict = {\n        \"pixel_values\": pixel_values_list,\n        \"labels\": input_ids_list\n    }\n    \n    dataset = Dataset.from_dict(dataset_dict)\n    \n    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:14.441924Z","iopub.execute_input":"2025-02-14T08:17:14.442252Z","iopub.status.idle":"2025-02-14T08:17:14.447502Z","shell.execute_reply.started":"2025-02-14T08:17:14.442225Z","shell.execute_reply":"2025-02-14T08:17:14.446526Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def collate_fn(batch):\n    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    \n    return {\n        \"pixel_values\": pixel_values,\n        \"labels\": labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:16.251075Z","iopub.execute_input":"2025-02-14T08:17:16.251360Z","iopub.status.idle":"2025-02-14T08:17:16.255473Z","shell.execute_reply.started":"2025-02-14T08:17:16.251338Z","shell.execute_reply":"2025-02-14T08:17:16.254532Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def configure_lora(model):\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:18.173232Z","iopub.execute_input":"2025-02-14T08:17:18.173548Z","iopub.status.idle":"2025-02-14T08:17:18.178302Z","shell.execute_reply.started":"2025-02-14T08:17:18.173523Z","shell.execute_reply":"2025-02-14T08:17:18.177140Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def train_model(model, dataset, processor):\n    training_args = TrainingArguments(\n        output_dir=\"./llama-vision-finetuned\",\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        weight_decay=0.01,\n        logging_steps=10,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        push_to_hub=False,\n        remove_unused_columns=False\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=collate_fn,\n    )\n    \n    trainer.train()\n    return trainer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:19.827195Z","iopub.execute_input":"2025-02-14T08:17:19.827487Z","iopub.status.idle":"2025-02-14T08:17:19.832532Z","shell.execute_reply.started":"2025-02-14T08:17:19.827464Z","shell.execute_reply":"2025-02-14T08:17:19.831536Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Prepare dataset\ndataset = prepare_dataset(cropped_dataset, true_texts, processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:25.986089Z","iopub.execute_input":"2025-02-14T08:17:25.986390Z","iopub.status.idle":"2025-02-14T08:17:54.075613Z","shell.execute_reply.started":"2025-02-14T08:17:25.986365Z","shell.execute_reply":"2025-02-14T08:17:54.074714Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Configure and fine-tune model\nmodel_finetuned = configure_lora(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T08:17:56.906464Z","iopub.execute_input":"2025-02-14T08:17:56.906812Z","iopub.status.idle":"2025-02-14T08:17:57.182818Z","shell.execute_reply.started":"2025-02-14T08:17:56.906776Z","shell.execute_reply":"2025-02-14T08:17:57.182046Z"}},"outputs":[{"name":"stdout","text":"trainable params: 11,796,480 || all params: 10,654,737,955 || trainable%: 0.1107\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"trainer, model_finetuned = train_model(model_finetuned, dataset, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_texts=extract_text(cropped_dataset,model_finetuned,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"organised_finetuned_texts=process_extracted_text(finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_wer, finetuned_cer = evaluate_texts(true_texts, organized_finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Baseline Model Performance:\")\nprint(f\"Word Error Rate: {baseline_wer:.4f}\")\nprint(f\"Character Error Rate: {baseline_cer:.4f}\")\n\nprint(\"\\nFine-tuned Model Performance:\")\nprint(f\"Word Error Rate: {finetuned_wer:.4f}\")\nprint(f\"Character Error Rate: {finetuned_cer:.4f}\")\n\nprint(\"\\nImprovement:\")\nprint(f\"Word Error Rate Improvement: {(baseline_wer - finetuned_wer) * 100:.2f}%\")\nprint(f\"Character Error Rate Improvement: {(baseline_cer - finetuned_cer) * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
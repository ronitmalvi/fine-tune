{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10725266,"sourceType":"datasetVersion","datasetId":6648899}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch accelerate bitsandbytes jiwer datasets peft loralib tqdm pytesseract opencv-python\n!apt-get install tesseract-ocr\n!apt-get install tesseract-ocr-eng","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-25T17:55:59.868642Z","iopub.execute_input":"2025-03-25T17:55:59.868962Z","iopub.status.idle":"2025-03-25T17:56:14.804442Z","shell.execute_reply.started":"2025-03-25T17:55:59.868931Z","shell.execute_reply":"2025-03-25T17:56:14.803329Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nCollecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nCollecting click>=8.1.8 (from jiwer)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nDownloading click-8.1.8-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, loralib, click, jiwer, bitsandbytes\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\nSuccessfully installed bitsandbytes-0.45.4 click-8.1.8 jiwer-3.1.0 loralib-0.1.2 rapidfuzz-3.12.2\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\n0 upgraded, 0 newly installed, 0 to remove and 117 not upgraded.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\ntesseract-ocr-eng set to manually installed.\n0 upgraded, 0 newly installed, 0 to remove and 117 not upgraded.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import ( LlamaForCausalLM, LlamaTokenizer, AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig )\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport jiwer\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom glob import glob\nimport random\nimport pytesseract\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:20.168527Z","iopub.execute_input":"2025-03-25T17:56:20.168866Z","iopub.status.idle":"2025-03-25T17:56:44.076160Z","shell.execute_reply.started":"2025-03-25T17:56:20.168839Z","shell.execute_reply":"2025-03-25T17:56:44.075489Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\n\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:45.970372Z","iopub.execute_input":"2025-03-25T17:56:45.971047Z","iopub.status.idle":"2025-03-25T17:56:46.170504Z","shell.execute_reply.started":"2025-03-25T17:56:45.971016Z","shell.execute_reply":"2025-03-25T17:56:46.169850Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:49.792286Z","iopub.execute_input":"2025-03-25T17:56:49.792721Z","iopub.status.idle":"2025-03-25T17:56:49.887277Z","shell.execute_reply.started":"2025-03-25T17:56:49.792686Z","shell.execute_reply":"2025-03-25T17:56:49.886238Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"\n\n# Load Images from the folder and Cropping them","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    for filename in os.listdir(folder):\n        if filename.endswith(\".jpg\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n    return images, image_names\n\nimage_folder = \"/kaggle/input/dataset/images\"\ndataset, image_names = load_images_from_folder(image_folder)\nprint(\"Loaded images successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:56:51.592824Z","iopub.execute_input":"2025-03-25T17:56:51.593175Z","iopub.status.idle":"2025-03-25T17:56:57.943583Z","shell.execute_reply.started":"2025-03-25T17:56:51.593150Z","shell.execute_reply":"2025-03-25T17:56:57.942781Z"}},"outputs":[{"name":"stdout","text":"Loaded images successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def augment_images(image_names):\n    for j,img in enumerate(image_names):\n        image_path = f\"/kaggle/input/dataset/images/{img}\"  # Change this to your image path\n        image = cv2.imread(image_path)\n        \n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        \n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n        clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n        \n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))  # Adjust based on text density\n        dilated = cv2.dilate(clean, kernel, iterations=2)\n        \n        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        bounding_boxes = [cv2.boundingRect(cnt) for cnt in contours]\n        bounding_boxes = sorted(bounding_boxes, key=lambda b: b[1])\n        \n        merged_boxes = []\n        i = 0\n        \n        while i < len(bounding_boxes) - 1:\n            x1, y1, w1, h1 = bounding_boxes[i]\n            x2, y2, w2, h2 = bounding_boxes[i + 1]\n        \n            # Check if two bounding boxes are close enough to be considered part of the same section\n            if abs(y2 - (y1 + h1)) < 50:  # Merge if next box is within 50 pixels\n                x_new = min(x1, x2)\n                y_new = min(y1, y2)\n                w_new = max(x1 + w1, x2 + w2) - x_new\n                h_new = max(y1 + h1, y2 + h2) - y_new\n        \n                merged_boxes.append((x_new, y_new, w_new, h_new))\n                i += 2\n            else:\n                i += 1\n        \n        output_dir = \"/kaggle/working/cropped_paragraphs\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        min_area = 10000  # Minimum bounding box area (to ignore single words)\n        min_height = 60   # Minimum height of a paragraph block\n        min_aspect_ratio = 0.5  # To avoid very long but short-height text (headers, footers)\n        \n        for i, (x, y, w, h) in enumerate(merged_boxes):\n            aspect_ratio = w / h \n        \n            if h > min_height and w * h > min_area and aspect_ratio > min_aspect_ratio:\n                cropped_paragraph = image[y:y+h, x:x+w]  # Crop paragraph\n                save_path = os.path.join(output_dir, f\"multi_paragraph_{j}_{i+1}.png\")\n                cv2.imwrite(save_path, cropped_paragraph)\n        \n                # cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        half_count = len(contours) // 3\n        j=1\n        for i, cnt in enumerate(contours):\n            x, y, w, h = cv2.boundingRect(cnt)\n            \n            \n            aspect_ratio = w / h \n        \n            if h > min_height and w * h > min_area and aspect_ratio > min_aspect_ratio:\n                cropped_paragraph = image[y:y+h, x:x+w]  # Crop paragraph\n                save_path = os.path.join(output_dir, f\"paragraph_{j}_{i+1}.png\")\n                cv2.imwrite(save_path, cropped_paragraph)\n                j=j+1\n            if j==half_count:\n                break\n                # Draw bounding box on original image (for visualization)\n                # cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:01.507196Z","iopub.execute_input":"2025-03-25T17:57:01.507524Z","iopub.status.idle":"2025-03-25T17:57:01.518227Z","shell.execute_reply.started":"2025-03-25T17:57:01.507499Z","shell.execute_reply":"2025-03-25T17:57:01.517299Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"augment_images(image_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:03.188311Z","iopub.execute_input":"2025-03-25T17:57:03.188615Z","iopub.status.idle":"2025-03-25T17:57:25.726621Z","shell.execute_reply.started":"2025-03-25T17:57:03.188591Z","shell.execute_reply":"2025-03-25T17:57:25.725703Z"},"scrolled":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Generate True text from the images using OCR\ntakes about 12-15 min to generate text","metadata":{}},{"cell_type":"code","source":"def load_images_from_folder(folder):\n    images = []\n    image_names = []\n    cnt=0\n    for filename in os.listdir(folder):\n        if filename.endswith(\".png\"):\n            img_path = os.path.join(folder, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            images.append(img)\n            image_names.append(filename)\n            cnt=cnt+1\n    return images, image_names,cnt\n\nimage_folder = \"/kaggle/working/cropped_paragraphs\"\ndataset, image_names,cnt = load_images_from_folder(image_folder)\nprint(f\"Loaded images successfully: {cnt}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:32.259600Z","iopub.execute_input":"2025-03-25T17:57:32.259921Z","iopub.status.idle":"2025-03-25T17:57:39.565980Z","shell.execute_reply.started":"2025-03-25T17:57:32.259895Z","shell.execute_reply":"2025-03-25T17:57:39.565223Z"}},"outputs":[{"name":"stdout","text":"Loaded images successfully: 1576\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate_ground_truth(images, image_names):\n    ground_truth = {}\n    for img, name in tqdm(zip(images, image_names), total=len(image_names), desc=\"Processing Images\"):\n        text = pytesseract.image_to_string(img).strip()\n        if not text:  # If OCR fails\n            text = \"N/A\"\n        ground_truth[name] = text\n    return ground_truth\n\nground_truth_data = generate_ground_truth(dataset, image_names)\nprint(\"true_texts generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:09:52.728599Z","iopub.execute_input":"2025-03-25T17:09:52.728978Z","iopub.status.idle":"2025-03-25T17:23:28.959409Z","shell.execute_reply.started":"2025-03-25T17:09:52.728949Z","shell.execute_reply":"2025-03-25T17:23:28.958382Z"}},"outputs":[{"name":"stderr","text":"Processing Images: 100%|██████████| 1576/1576 [13:36<00:00,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"true_texts generated\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Creating and Loading the model \"Llama-3.2-11B-Vision\"\nLoaded a 4-bit quantized llama-3.2-11B-vision model. *Note: Loading the model can take upto 4 minutes as the model is 22GB big to download*","metadata":{}},{"cell_type":"code","source":"def load_model():\n    model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n\n    processor = AutoProcessor.from_pretrained(model_name)\n\n    return model, processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:45.535869Z","iopub.execute_input":"2025-03-25T17:57:45.536207Z","iopub.status.idle":"2025-03-25T17:57:45.540677Z","shell.execute_reply.started":"2025-03-25T17:57:45.536180Z","shell.execute_reply":"2025-03-25T17:57:45.539861Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model, processor = load_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T17:57:47.993260Z","iopub.execute_input":"2025-03-25T17:57:47.993562Z","iopub.status.idle":"2025-03-25T18:02:10.207869Z","shell.execute_reply.started":"2025-03-25T17:57:47.993540Z","shell.execute_reply":"2025-03-25T18:02:10.207096Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/5.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1b6ad488a94247b06c5cf024a2cfc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"879938115a184aa7bde5738da1759a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d887b634254e91adb9d8a67c280844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"998c9f60d265421987415f2f5e0e1303"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ddd351a624492495757312a12d3050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbfc7aa188a6497294296a8a6b81f54c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2520b8d5bb80484b94bba5ca196c1e38"}},"metadata":{}},{"name":"stderr","text":"Error while downloading from https://cdn-lfs-us-1.hf.co/repos/f2/7e/f27e26d7824ae3c888e292f98ea8166ad1843be96bc0fb64235bda4c0030da7b/806d7a1d87d0a2d45b2f6c42dddc8b8f2b77bcaf45ac085181d0af74f7492909?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00004-of-00005.safetensors%3B+filename%3D%22model-00004-of-00005.safetensors%22%3B&Expires=1742929185&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjkyOTE4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2YyLzdlL2YyN2UyNmQ3ODI0YWUzYzg4OGUyOTJmOThlYTgxNjZhZDE4NDNiZTk2YmMwZmI2NDIzNWJkYTRjMDAzMGRhN2IvODA2ZDdhMWQ4N2QwYTJkNDViMmY2YzQyZGRkYzhiOGYyYjc3YmNhZjQ1YWMwODUxODFkMGFmNzRmNzQ5MjkwOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=FRnEI71GPYBvuvvOOlChZHdxS42otFryRj4wR76AzW03zN0a%7EpXPAxRYt6d65M5PuEZAsnqVc4LdB2X%7Eec5ecRD0h7D%7EMHiUiQ1qBOWx35zdyeZNvN5LxgC82fN1xUsyDHGY5dYt8ppiyupfEPiAloHj0Q7wLYJ-3pdTA9pAqZW9%7EZMwTsEewMvGDzKEvqOvl5yuBXH6ktD41aBNWkUSy9GqNIgUnu6Hsf-ALGAz04szRzDmnPFpydz5WR89HRxM891KM3PTlO-rDCYGCa26etnfNYiE9r7WMx1uajpH48qZEGrIPzA0ZW1E705QBG-u6nmZQ838SnDQvFK-oD4jMg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\nTrying to resume download...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:  75%|#######5  | 3.76G/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17acef0ed37e4bd68a1023273b47907e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e232ec91a8a24695b6a2b8e456187244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c792a2b0b7493fa23995405400f358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c56f22ade8d4e1db24cecca82628d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ee1427de7346a291b010ad200081b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ff4797015043d7b616886ba1902d53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd8f6bc13f0442a95ff4a4c22b684b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f43d7bd8044374adb8e0aa4d6b1d48"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"#  Extract baseline texts using the model\n* Took about 1.5 hours to extract text using the model if the max_length put to be 256.\n* And would take 3 hours for max_length=512.","metadata":{}},{"cell_type":"code","source":"from torch.amp import autocast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:29.762602Z","iopub.execute_input":"2025-03-25T18:02:29.763309Z","iopub.status.idle":"2025-03-25T18:02:29.767146Z","shell.execute_reply.started":"2025-03-25T18:02:29.763278Z","shell.execute_reply":"2025-03-25T18:02:29.766088Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:30.838082Z","iopub.execute_input":"2025-03-25T18:02:30.838455Z","iopub.status.idle":"2025-03-25T18:02:30.842108Z","shell.execute_reply.started":"2025-03-25T18:02:30.838426Z","shell.execute_reply":"2025-03-25T18:02:30.841302Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def extract_text(images, model, processor, batch_size=6):\n    texts = []\n    \n    device = next(model.parameters()).device\n    \n    for i in tqdm(range(0, len(images), batch_size)):\n        batch = images[i:i+batch_size]\n        \n        inputs = processor(images=batch, return_tensors=\"pt\", padding=True).to(device)\n        \n        with autocast('cuda'):\n            outputs = model.generate(**inputs, max_length=128, num_beams=2, early_stopping=True)\n        \n        batch_texts = processor.batch_decode(outputs, skip_special_tokens=True)\n        texts.extend(batch_texts)\n    \n    return texts\n\nbaseline_texts = extract_text(dataset, model, processor, batch_size=6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T18:02:34.066669Z","iopub.execute_input":"2025-03-25T18:02:34.067022Z","iopub.status.idle":"2025-03-25T18:02:47.471922Z","shell.execute_reply.started":"2025-03-25T18:02:34.066978Z","shell.execute_reply":"2025-03-25T18:02:47.470514Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/263 [00:13<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-07b2445ff025>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbaseline_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-07b2445ff025>\u001b[0m in \u001b[0;36mextract_text\u001b[0;34m(images, model, processor, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2283\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2284\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2285\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3503\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mllama/modeling_mllama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, aspect_ratio_mask, aspect_ratio_ids, attention_mask, cross_attention_mask, cross_attention_states, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;31m# get vision tokens from vision model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m             vision_outputs = self.vision_model(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 \u001b[0maspect_ratio_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect_ratio_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mllama/modeling_mllama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;31m# Concatenate final hidden state and intermediate hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_hidden_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 14.74 GiB of which 862.12 MiB is free. Process 3855 has 13.90 GiB memory in use. Of the allocated memory 12.38 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 14.74 GiB of which 862.12 MiB is free. Process 3855 has 13.90 GiB memory in use. Of the allocated memory 12.38 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"# Text Organization\nCleaning the texts using regular expression for removing some unnecessary characters","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt_tab')\n\ndef clean_text(text):\n    text = text.strip()  # Remove leading and trailing spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n    text = re.sub(r'\\n+', ' ', text)  # Remove excessive newlines\n    text = text.replace(\"  \", \" \")  # Remove double spaces\n    return text\n\ndef structure_text(text):\n    sentences = sent_tokenize(text)  # Tokenize into sentences\n    structured_text = \"\\n\".join(sentences)  # Join sentences with newline\n    return structured_text\n\ndef process_extracted_text(extracted_texts):\n    organized_texts = []\n    for text in extracted_texts:\n        cleaned = clean_text(text)\n        structured = structure_text(cleaned)\n        organized_texts.append(structured)\n    \n    return organized_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_list(data, batch_size):\n    return [\"\".join(data[i:i + batch_size]) for i in range(0, len(data), batch_size)]\n    \nbatch_true_text=batch_list(true_texts,4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_true_texts = process_extracted_text(batch_true_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_predicted_texts=process_extracted_text(baseline_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating texts i.e. calculating words error and character error\n* The error comes to be quite high because the length of the text generated by the model is much less than the length of actual text as extrated by OCR.","metadata":{}},{"cell_type":"code","source":"!pip install jiwer\nfrom jiwer import wer, cer\ndef evaluate_texts(true_texts, predicted_texts):\n    word_error = wer(true_texts, predicted_texts)\n    char_error = cer(true_texts, predicted_texts)\n    return word_error, char_error","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_wer,baseline_cer = evaluate_texts(true_texts, predicted_texts)\nprint(f\"Word Error Rate: {baseline_wer}\")\nprint(f\"Character Error Rate: {baseline_cer}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tune the model using LoRA (Low Rank Adaptation)\n* training the model and obtaining the fine_tuned model took quite longer approax. 6-7 hours ","metadata":{}},{"cell_type":"code","source":"# def prepare_dataset(images, true_texts, processor):\n#     pixel_values_list = []\n#     input_ids_list = []\n    \n#     for img, text in zip(images, true_texts):\n#         image_features = processor(images=img, return_tensors=\"pt\")\n#         pixel_values_list.append(image_features[\"pixel_values\"][0])\n        \n#         text_features = processor(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=256)\n#         input_ids_list.append(text_features[\"input_ids\"][0])\n    \n#     dataset_dict = {\n#         \"pixel_values\": pixel_values_list,\n#         \"labels\": input_ids_list\n#     }\n    \n#     dataset = Dataset.from_dict(dataset_dict)\n    \n#     dataset = dataset.train_test_split(test_size=0.1, seed=42)\n#     return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset_batch(images, true_texts, processor, batch_size=4):\n    pixel_values = []\n    input_ids = []\n    \n    # Process images in batches\n    for i in range(0, len(images), batch_size):\n        batch_images = images[i:i+batch_size]\n        batch_texts = [\"\".join(true_texts[i:i + batch_size])]\n        \n        # Process image batch\n        image_features = processor(\n            images=batch_images, \n            return_tensors=\"pt\",\n            padding=True\n        )\n        pixel_values.extend(image_features[\"pixel_values\"])\n        \n        # Process text batch\n        text_features = processor(\n            text=batch_texts,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n        input_ids.extend(text_features[\"input_ids\"])\n        \n    print(len(pixel_values))\n    print(len(input_ids))\n    return Dataset.from_dict({\n        \"pixel_values\": pixel_values,\n        \"labels\": input_ids\n    }).train_test_split(test_size=0.1, seed=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def collate_fn(batch):\n#     pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n#     labels = torch.stack([item[\"labels\"] for item in batch])\n    \n#     return {\n#         \"pixel_values\": pixel_values,\n#         \"labels\": labels\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def configure_lora(model):\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train_model(model, dataset, processor):\n#     training_args = TrainingArguments(\n#         output_dir=\"./llama-vision-finetuned\",\n#         num_train_epochs=3,\n#         per_device_train_batch_size=8,\n#         per_device_eval_batch_size=8,\n#         gradient_accumulation_steps=2,\n#         dataloader_num_workers=2,  # parallel loading\n#         learning_rate=2e-4,\n#         weight_decay=0.01,\n#         logging_steps=10,\n#         eval_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         load_best_model_at_end=True,\n#         push_to_hub=False,\n#         remove_unused_columns=False\n#     )\n    \n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=dataset[\"train\"],\n#         eval_dataset=dataset[\"test\"],\n#         data_collator=collate_fn,\n#     )\n    \n#     trainer.train()\n#     return trainer, model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback, TrainingArguments, Trainer\nfrom tqdm import tqdm\n\nclass ProgressBarCallback(TrainerCallback):\n    def __init__(self, total_steps):\n        self.pbar = tqdm(total=total_steps, desc=\"Training Progress\")\n\n    def on_step_end(self, args, state, control, **kwargs):\n        self.pbar.update(1)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        self.pbar.close()\n\ndef train_model(model, dataset, processor):\n    training_args = TrainingArguments(\n        output_dir=\"./llama-vision-finetuned\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=2,\n        dataloader_num_workers=2,\n        learning_rate=2e-4,\n        weight_decay=0.01,\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        push_to_hub=False,\n        remove_unused_columns=False\n    )\n\n    total_steps = (len(dataset[\"train\"]) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n    progress_callback = ProgressBarCallback(total_steps)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=collate_fn,\n        callbacks=[progress_callback]  # Attach the progress bar\n    )\n\n    trainer.train()\n    return trainer, model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset\ndataset = prepare_dataset_batch(cropped_dataset, true_texts, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure and fine-tune model\nmodel_finetuned = configure_lora(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, model_finetuned = train_model(model_finetuned, dataset, processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_texts=extract_text(cropped_dataset,model_finetuned,processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"organised_finetuned_texts=process_extracted_text(finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"finetuned_wer, finetuned_cer = evaluate_texts(true_texts, organized_finetuned_texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Baseline Model Performance:\")\nprint(f\"Word Error Rate: {baseline_wer:.4f}\")\nprint(f\"Character Error Rate: {baseline_cer:.4f}\")\n\nprint(\"\\nFine-tuned Model Performance:\")\nprint(f\"Word Error Rate: {finetuned_wer:.4f}\")\nprint(f\"Character Error Rate: {finetuned_cer:.4f}\")\n\nprint(\"\\nImprovement:\")\nprint(f\"Word Error Rate Improvement: {(baseline_wer - finetuned_wer) * 100:.2f}%\")\nprint(f\"Character Error Rate Improvement: {(baseline_cer - finetuned_cer) * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}